---
title: DRAFT Practical Data Stewardship for Salmon Biologists--A Blueprint for Domain-Specific Best Practices in Fisheries DRAFT
  
authors:
  - name: Brett Johnson
    affiliation: Fisheries and Oceans Canada
    roles: writing - original draft, conceptualization, data curation, project administration, investigation, methodology
    corresponding: true
  - name: Scott Akenhead
    affiliation: TBD
    roles: writing - original draft, conceptualization
  - name: Katie Barnas
    affiliation: National Oceanic and Atmospheric Administration
    roles: writing - review and editing
  - name: Jennifer Bayer
    affiliation: U.S. Geological Survey
    roles: writing - original draft, conceptualization, project administration
  - name: Tomas Bird
    affiliation: Fisheries and Oceans Canada
    roles: writing - original draft, conceptualization
  - name: Samuel Cimino
    affiliation: Pacific States Marine Fisheries Commission
    roles: writing - original draft
  - name: Graeme Diack
    affiliation: Missing Salmon Alliance
    roles: writing - review and editing, conceptualization
  - name: Lara Erikson
    affiliation: Pacific States Marine Fisheries Commission
    roles: conceptualization, writing - review and editing
  - name: Nancy Leonard
    affiliation: Pacific States Marine Fisheries Commission
    roles: conceptualization, writing - original draft
  - name: Catherine Michielsens
    affiliation: Pacific Salmon Commission
    roles: writing - review and editing
  - name: Fiona Martens
    affiliation: Pacific Salmon Commission
    roles: writing - review and editing
  - name: Emily Lescak
    affiliation: University of Alaska Fairbanks
    roles: writing - review and editing
  - name: Gottfried Pestal
    affiliation: Solv Consulting
    roles: writing - review and editing
  - name: Matt Jones
    affiliation: National Centre of Ecological Analysis and Synthesis
    roles: writing - review and editing
  - name: Mark Saunders
    affiliation: North Pacific Anadromous Fish Commission
    roles: conceptualization, writing - review and editing
  - name: Yi Xu
    affiliation: Washington Department of Fish and Wildlife
    roles: writing - review and editing
    
bibliography: references.bib
appendix-style: plain
---

**Keywords:** Salmon data stewardship, data interoperability, FAIR principles, persistent identifiers (PIDs), controlled vocabularies, metadata standards, application programming Interface (API), data citation, ontology development

To do items:

- Discuss differences between data management plans, data governance charters, data sharing agreements
- Incorporate ref to Streamnet Data Exchange Standards somehow
- Add in figures
- fill out appendix 1 more thoroughly
- Refine Reorg the content in appendix 2 (traning roadmap) and decide if it Makes sense to put some of that content into a 3rd column in table 1
- Abstract refinement

### Abstract

Fisheries research, management, and conservation increasingly generate vast and diverse data crucial for timely decision-making. Yet these data remain largely fragmented across jurisdictions, disciplines, and outdated infrastructure, limiting their use in responsive fisheries management. Biologists are increasingly taking on data stewardship responsibilities to address these challenges, often without clear guidance, training, or support. Shared, community-agreed practices for implementing domain-specific data standards are needed to move beyond generic data management guidance toward fit-for-purpose tools and workflows. To address this gap—and to show how other communities can do so—we develop seven practices for salmon data stewardship and demonstrate their application through a real-world case study. We provide practical guidance for those transitioning into these essential stewardship roles, including domain-specific tools, templates, and examples from salmon research and management. We argue that effective salmon management depends on formally establishing data stewardship as a dedicated, institutionally supported professional role. These practices integrate both sociocultural and technical approaches to ensure data meet modern open science principles and respect Indigenous Data Sovereignty. Through a case study of a historical sockeye salmon productivity analysis spanning Pacific Coast jurisdictions, we highlight how clearly defined data stewardship practices enhance data reproducibility, integration, and management efficacy. With a foundation of shared practices, data stewards will enable faster, more transparent decision-making, support development of machine-actionable datasets with high-quality metadata and consistent semantics that enable automated analysis, and expand the use of cross-jurisdictional datasets—ultimately strengthening the management and conservation of salmon populations and the ecosystems they inhabit, and, by extension, other data-rich fisheries data domains.

## The Data Stewardship Challenge

Integrated, timely, and high-quality data are essential for effective fisheries research, management, and conservation. Such data underpin robust stock assessments, inform adaptive management strategies, enable rapid responses to emerging threats, and support transparent decision-making. Yet across the fisheries domain, biologists face persistent challenges in achieving these goals. Data on fish populations, health, and environmental conditions are often fragmented, inconsistently measured, and incomplete across time, space, and life-history stages [@noaadatagovernancecommittee2024]. These issues limit the utility of fisheries data for research and management.

### Cross-Jurisdictional Data Fragmentation

The challenges are especially pronounced in salmon science, where data must be integrated across multiple ecological regions and jurisdictional boundaries [@groot1991]. Salmon biologists routinely collect information managed by diverse agencies and institutions, often in isolation and without a focus on interoperability. While localized successes in data coordination exist---particularly within regional fisheries management offices and treaty commissions---salmon data integrated across agencies for each phase of the salmon life cycle is uncommon and costly for most programs [@marmorek2011; @inman2021; @diack2024]. Most salmon datasets remain confined within institutional silos, often undocumented, stored in outdated systems, or formatted according to internal standards that are incompatible with broader integration efforts. Even within organizations, data can be siloed by data type or program with freshwater data going in one data system while estuary, open-ocean, and commercial fishery data each housed in separate data systems with limited ability to easily re-connect the data through shared identifiers.

This fragmentation is compounded by the number of disciplines and organizations involved. Geneticists, oceanographers, freshwater ecologists, stock assessment biologists, and fisheries managers all contribute data using their own field-specific conventions and workflows. Meanwhile, data is distributed across federal, state, provincial, tribal, and academic institutions---each with its own mandates, technologies, and metadata requirements. Many salmon data-holding organizations rely on aging infrastructure or opaque, undocumented standards that lag behind modern open-science practices. This tangle of disciplinary and institutional fragmentation slows integration, hinders reproducibility, and delays analyses that could otherwise inform time-sensitive management decisions, conservation actions, or restoration plans. When critical datasets are hard to find, access, or interpret, biologists and analysts lose valuable time trying to reconstruct or harmonize them, reducing transparency, increasing the risk of errors, and delaying urgent conservation or management responses.

### The Need for Coordinated Stewardship

The growing complexity of fisheries management, combined with escalating environmental uncertainties due to climate change, demands rapid, integrated, and robust data analyses [@bull2022, ward_surveyjoin_2025]. The mismatch between fragmented data systems and fixed administrative boundaries creates an urgent need for interoperable, dynamic, multi-scale data stewardship that can adapt to shifting ecological and management priorities. Despite the scale and importance of these datasets, biologists who collect and manage salmon data are often expected to act as de facto data stewards without training, guidance, institutional support, or access to community-agreed best practices. Tasks such as documenting methods, aligning terminology, formatting for data sharing, and publishing data are typically performed off the side of a biologist's desk. A lack of institutional support [@diack2024], training [@volk2014], and dedicated roles for data management further relegate critical data stewardship tasks to an *ad hoc* status. The absence of clear roles, standards, and community-endorsed practices leaves even motivated scientists unsure how to structure their data for future use. As a result, data stewardship is inconsistent and reactive, and data integration remains a major bottleneck to adaptive management and ecosystem-scale learning.

Both researchers and managers struggle to align their data with community-agreed principles such as FAIR (Findable, Accessible, Interoperable, and Reusable) [@wilkinson2016] and Indigenous Data Sovereignty frameworks like the CARE principles (Collective Benefit, Authority to Control, Responsibility, and Ethics) [@carroll2019; @jennings2023a]. Adhering to CARE data management principles is all the more important when it comes to salmon related data given the sociocultural importance of salmon to First Nations, Tribes, and Indigenous communities throughout the North Pacific and North Atlantic regions [@ween2013; @eartheconomics2021]. Large volumes of data collected through long-term monitoring programs hold tremendous value, especially for secondary users---but are often inaccessible due to a lack of time, resources, and incentives for data producers to publish them [@lindenmayer2012]. Without clear support and guidance, well-intentioned practitioners are left with ad hoc approaches that limit reuse and interoperability. This gap can only be bridged by equipping both data producers and stewards with tools, support, and institutional backing to publish interoperable, machine-readable metadata and datasets in alignment with shared principles.

### Framework for Action

In this paper, we provide actionable practices, examples, and workflows to help salmon biologists improve the usability, reproducibility, and long-term impact of their data. We develop seven best practices for salmon data stewardship and demonstrate their application through a real-world case study of cross-jurisdictional sockeye productivity analysis. Our case study shows how cross-jurisdictional alignment of terms and reproducible pipelines can enable faster status assessment updates and more responsive management decisions. Our goal is to support salmon biologists and the broader research and management community to effectively steward salmon data. To keep this broadly useful, we emphasize patterns—lifecycle planning, metadata governance, vocabulary alignment, reproducible publishing, and role clarity—that any taxa-centric community can adopt, substituting their own standards and tools. We also map the seven practices to widely used data-lifecycle models to make adoption straightforward outside salmon contexts.

A coordinated approach to stewarding salmon data should follow established open science standards and adhere explicitly to FAIR principles, tailored specifically for salmon research and management. Our practices build upon existing standards and vocabularies including Darwin Core, OBIS, schema.org, and OBO Foundry ontologies, ensuring compatibility with broader biodiversity informatics infrastructure rather than reinventing foundational frameworks. Achieving meaningful interoperability demands both breadth and depth. **Broad interoperability** integrates diverse scientific domains, systems, and formats, requiring structured, machine-readable data and metadata published openly for maximum discoverability. **Deep interoperability** demands precise definitions of salmon-specific terms and methods, ensuring data remains meaningful and usable across contexts. Salmon data stewards can improve conservation outcomes for salmon by coordinating across boundaries to develop a shared foundation of data stewardship practices. To address these foundational challenges, we must establish clear data stewardship roles and practices that span the entire data lifecycle—from collection and documentation through integration and long-term preservation.

## Defining Data Stewardship in Salmon Science

Data stewardship encompasses the coordinated practices, roles, and responsibilities necessary to effectively manage, share, and reuse data throughout its lifecycle [@environm2007; @plotkin2014; @peng2018]. Within fisheries science, stewardship involves ensuring data quality, compliance with agreed-upon standards, and the establishment of clear governance to guide data collection, documentation, integration, and preservation. However, salmon data stewardship goes beyond mere technical data management; it involves actively facilitating collaboration, communication, and consensus-building among data producers and users across multiple institutions and jurisdictions.

Data stewardship represents a critical subdiscipline within the broader field of data science. While data science is often narrowly associated with machine learning and statistical modeling, we adopt a more comprehensive view that encompasses how we treat, handle, and represent data, along with the social and technical information systems that enable data use for science. Data stewardship focuses on the practical implementation of these principles—ensuring that data infrastructure, standards, and practices actually serve scientific and management needs rather than remaining theoretical constructs.

Effective salmon data stewards serve as boundary spanners and community managers, convening diverse stakeholders across agencies, First Nations, Tribes, and academic institutions to build sustained communities of practice. This boundary-spanning role is particularly critical in transboundary contexts where data integration requires navigating complex jurisdictional and cultural boundaries [@ward_surveyjoin_2025]. By facilitating communication, translating between different organizational cultures and technical systems, and maintaining long-term relationships, data stewards create the social infrastructure necessary for effective cross-boundary data collaboration.

::: {.callout-note}
## Box 1: Critical Functions of Salmon Data Stewards

Effective salmon data stewards perform several critical functions:

-   **Technical oversight**: Ensuring metadata completeness, adherence to standardized terminologies and vocabularies, and robust quality assurance protocols.

-   **Social and organizational facilitation**: Leading stakeholder engagement, capacity-building activities, and negotiation of data access and sharing agreements, including addressing First Nations, Tribes, and Indigenous Peoples' rights and interests in data governance.

-   **Institutional advocacy**: Championing the institutional recognition of data stewardship roles, promoting sustained investment and dedicated resources for data management infrastructure and practices.

-   **Implementation and adoption facilitation**: Actively promoting data use and ensuring that standards and practices remain practical and relevant by maintaining close contact with real-world applications. This includes monitoring data utilization, gathering feedback from users, and iteratively refining standards based on actual implementation challenges to prevent theoretical approaches that fail in practice.
:::

Data stewards can implement FAIR and CARE principles through concrete technical and governance mechanisms they control, such as documenting consent constraints and access levels in metadata, using controlled vocabularies to ensure consistent terminology, and establishing repository roles that enforce data sovereignty requirements. For example, stewards can document consent constraints in metadata fields and enforce access restrictions via repository user roles, ensuring that Indigenous data sovereignty is respected while maintaining data discoverability and appropriate reuse. This governance approach is particularly critical for sensitive data such as Traditional Knowledge and sensitive habitat locations, where stewardship practices must balance open science principles with appropriate access controls and cultural protocols.

 Data stewards play a critical role bridging the gap between biologists and Information Technology (IT) staff by translating data needs into application or data system features. A user-centred design approach to salmon data stewardship is critical and focuses on creating tools that align with biologists' needs. When data management is separated from biologists, accountability weakens, and quality issues go unnoticed. While IT expertise is essential for infrastructure and security, effective data system design requires IT to act as an enabler, rather than gatekeeper, provisioning self-serve data infrastructure. The Data Steward, serving as a translator between IT and biologists, enables biologists to engage independently with data systems, fostering ownership and accountability and ultimately improving data quality for research and management.

Dedicated stewardship roles empower salmon biologists to bridge disciplinary divides and jurisdictional barriers, transforming fragmented datasets into cohesive, interoperable resources. By proactively defining, implementing, and maintaining data standards and workflows, salmon data stewards create conditions for timely, accurate, and reproducible analyses. Such stewardship positions salmon biologists to better inform adaptive management decisions, ultimately strengthening salmon conservation and resilience.

## Updating Pacific-wide Sockeye Productivity: A Case Study for What Agencies Could Do Now

This case study revisits a Pacific Coast-wide sockeye productivity dataset assembled from diverse agency sources by academic researchers [@peterman2012]. We reflect not on the significant work the research team accomplished, but rather on the preventable institutional and technical barriers that impeded their work---and continue to burden data updates and reuse efforts today. Their study examined productivity trends across 64 sockeye salmon stocks spanning Washington, British Columbia (B.C.), and Alaska. However, attempting to replicate or build upon this analysis today is an arduous, time-consuming, and error-prone endeavour due to fragmented data sources, inconsistent formats, and lack of standardized practices among the key institutions involved: the Washington Department of Fish and Wildlife (WDFW), Fisheries and Oceans Canada (DFO), and the Alaska Department of Fish and Game (ADF&G).

Each section below highlights a key challenge faced by the team and proposes practical steps based on our best practices (@tbl-bestpractices) that data-holding agencies could do to enable easier integration, validation, and updating of salmon datasets across jurisdictions and decades. This case study illustrates how implementing the foundational concepts and practical recommendations outlined in this paper can transform data stewardship practices within these organizations. By doing so, they can significantly enhance data accessibility, quality, and interoperability---ultimately enabling more efficient and accurate analyses that support salmon conservation and management.

### Challenge 1: Interpreting the Data --- What do these numbers actually mean?

Peterman's team frequently worked with datasets that lacked basic contextual information. Fields such as "year," "return," or "age class" were often undefined or inconsistently used. For example, some datasets recorded returns by calendar year while others used brood year, and few included metadata to clarify the distinction. In many cases, the team had to reconstruct metadata by back-checking against reports or simulating assumptions (e.g., about age structure) to interpret the data correctly.

**Remedies:**

-   **Best Practice 3: Make Data, People, Projects, and Outputs Discoverable, Linked and Citable with Persistent Identifiers (PIDs).** Assigning PIDs such as digital object identifiers (DOIs) to protocols, methods, and people (via ORCIDs) and linking them together using data stores and catalogues links data to its provenance and ensures that methods, context, and interpretation decisions are traceable.

-   **Best Practice 4. Use Shared Data Models, Vocabularies and Metadata to Enable Integration.** To prevent this kind of ambiguity, agencies can now adopt internationally recognized metadata schemas such as ISO 19115 or Ecological Metadata Language, data models (Darwin Core Data Package) to model age and age type data concepts, and use controlled vocabularies to restrict the permissible values in the age field to calendar year, brood year, or otherwise.

### Challenge 2: Accessing and Using the Data --- Where is it stored, and how do I get it?

The Peterman dataset was compiled from multiple files scattered across email inboxes, regional offices, and grey literature. Data were stored in inconsistent formats, lacked clear versioning, and were difficult to discover outside of specific research networks. Even today, no API or structured access mechanism exists to update or query the data programmatically. As a result, researchers hoping to build on the dataset may have to start from scratch.

**Remedies:**

-   **Best Practice 2: Reuse Proven Infrastructure to Save Time and Increase Interoperability**\
    Rather than developing bespoke data catalogues or repositories, agencies should adopt existing catalogues used beyond their own institution such as the Ocean Biodiversity Information System, Zenodo, or the Knowledge Network for Biocomplexity). These are proven platforms with a broad user base that support persistent storage, discoverability, and interoperability.

-   **Best Practice 5: Store and Analyze Data in Ways That Others Can Easily Access, Use, and Trust**\
    Agencies can use open-access data repositories or their own institutional data repositories or catalogues that make data discoverable using PIDs and provide programmatic access to data possible using Application Programming Interfaces.

### Challenge 3: Sustaining the Dataset --- Who is responsible, and why should I contribute?

Once Peterman and his team completed their analysis, no formal plan existed for sustaining or updating the dataset. Responsibility for ongoing maintenance fell informally to former students and collaborators. Despite its national and international relevance, the dataset was never adopted by an agency as a living product. Moreover, the original data contributors often lacked incentives, support, or recognition for their efforts---conditions that persist in many data environments today.

**Remedies:**

-   **Best Practice 1: Make Data Governance Explicit to Support Trust and Reuse** Agencies should define roles, responsibilities, and decision-making processes through formal governance mechanisms such as data product charters. Use a Data Management Plan with a responisibility matrix such as "responsible, approver, consulted, informed" (RACI) to clarify govermamce, assign maintenance responsibility, and ensure continuity across staff turnover and institutional change.

-   **Best Practice 6: Incentivize and Track Data Sharing and Reuse** Visibility, credit, and metrics are critical for motivating data sharing. Agencies can embed citation guidance in metadata and track dataset reuse through COUNTER-compliant dashboards or DataCite APIs.

-   **Best Practice 7: Build Community Through Co-Development and Mutual Benefit** Effective data stewardship requires collaboration between biologists, First Nations, Tribes, Indigenous communities, managers, and data professionals. Participatory design ensures that systems and standards meet user needs and are adopted over time. *Practical application:* Facilitate cross-jurisdictional working groups to co-develop data standards and align on shared outcomes for priority datasets.

While the analytical contribution of the Peterman productivity dataset remains significant, the barriers encountered in compiling, interpreting, and maintaining the data are instructive. These challenges are not unique to Peterman's team---they reflect systemic gaps in data governance, documentation, infrastructure, and incentives. By adopting the seven best practices detailed in @tbl-bestpractices, agencies and researchers can transform legacy datasets into living resources, enabling reproducibility, easing collaboration, and accelerating insight across the salmon research and management community.

The challenges and solutions demonstrated in this salmon case study generalize across fisheries and environmental monitoring domains. Cross-jurisdictional data harmonization, quality assurance and control patterns, standardized metadata requirements, and long-term archiving strategies are universal needs that extend far beyond salmon science. Similar barriers and solutions apply to trawl survey data integration, invertebrate monitoring programs, and water quality datasets that span multiple agencies and jurisdictions.

### How our seven practices align to data lifecycle models

Our seven best practices map directly to established data lifecycle models, demonstrating their broad applicability beyond salmon science. The NOAA Data Lifecycle provides a widely recognized framework with six sequential stages (Plan, Obtain, Process, Preserve, Access, Disposition) and four cross-cutting elements (Document, Track and Monitor, Quality, Security) [@noaadatagovernancecommittee2024]. This alignment ensures our practices are grounded in established federal data management standards and can be readily adopted by other agencies and research communities.

The mapping shown in @tbl-lifecycle demonstrates how each practice addresses specific lifecycle stages while the cross-cutting elements ensure comprehensive data stewardship throughout the entire lifecycle. For example, Practice 1 (Data Governance) spans the entire lifecycle from planning through disposition, while Practice 4 (Shared Data Models) primarily supports the Process and Preserve stages. This systematic alignment with established frameworks enhances the credibility and portability of our approach across different domains and institutions.

+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+
| **Best Practice** | **Plan** | **Obtain** | **Process** | **Preserve** | **Access** | **Disposition** | **Cross-cutting** |
+:=================:+:========:+:=========:+:=========:+:==========:+:=========:+:============:+:================:+
| **1. Data Governance** | ● | ● | ● | ● | ● | ● | Document, Quality |
+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+
| **2. Reuse Infrastructure** | ● | | | ● | ● | | Track and Monitor |
+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+
| **3. Persistent Identifiers** | ● | | ● | ● | ● | | Document, Track |
+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+
| **4. Shared Data Models** | ● | | ● | ● | | | Quality |
+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+
| **5. Accessible Storage** | | | ● | ● | ● | | Security, Quality |
+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+
| **6. Incentivize Sharing** | ● | | | ● | ● | | Track and Monitor |
+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+
| **7. Community Building** | ● | ● | ● | ● | ● | ● | Document, Quality |
+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+

: Mapping of seven best practices to NOAA Data Lifecycle stages and cross-cutting elements {#tbl-lifecycle}

### Metadata governance as a cross-cutting foundation

Unlike the sequential stages of the data lifecycle, metadata governance operates as a continuous, cross-cutting practice that spans all phases simultaneously. While data moves through Plan → Obtain → Process → Preserve → Access → Disposition, metadata governance must be active throughout, ensuring that documentation, quality control, and discoverability are maintained at every stage. This cross-cutting nature means that metadata governance failures at any point can compromise the entire data stewardship effort, making it a critical foundation rather than a discrete step in the process.

The lifecycle mapping in @tbl-lifecycle reveals that data governance elements appear in every stage: planning metadata requirements (Plan), documenting collection methods (Obtain), structuring and validating metadata (Process), ensuring long-term preservation (Preserve), enabling discovery and access (Access), and managing final disposition (Disposition). This pervasive presence underscores why metadata governance must be treated as an institutional capability rather than a project-specific task, requiring dedicated resources, trained personnel, and systematic processes that operate continuously across all data activities.

+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| **Best Practice**                                                                                                                                                                                                                                                                                                                  | **Practical Applications**                                                                                                            |
+:===================================================================================================================================================================================================================================================================================================================================+:======================================================================================================================================+
| **1. Make Data Governance Explicit to Support Trust and Reuse.** Establishing clear governance structures ensures quality, accountability, and compliance with FAIR and CARE principles. It enables trust and long-term stewardship across multi-organizational projects.                                                          | \- Document roles and responsibilities using a Data Product Governance Charter and structured frameworks (e.g., DACI or RACI).\       |
|                                                                                                                                                                                                                                                                                                                                    | - Integrate CARE principles to respect First Nations, Tribes, and Indigenous data rights.\                                                                       |
|                                                                                                                                                                                                                                                                                                                                    | - Form a governance or oversight committee to review data standards, timelines, and agreements.                                       |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| **2. Reuse Proven Infrastructure to Save Time and Increase Interoperability.** Leveraging existing platforms and technologies reduces costs and improves long-term interoperability and sustainability.                                                                                                                            | \- Use domain-specific repositories like OBIS or GBIF.\                                                                               |
|                                                                                                                                                                                                                                                                                                                                    | - Publish and archive data with KNB or Zenodo.                                                                                        |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| **3. Make Data, People, Projects, and Outputs Discoverable, Linked and Citable with PIDs.** Persistent identifiers (PIDs) connect data with researchers, institutions, and outputs---supporting data citation, reuse, and automated attribution.                                                                                   | \- Encourage use of ORCID iDs for researchers.\                                                                                       |
|                                                                                                                                                                                                                                                                                                                                    | - Use ROR IDs for institutions.\                                                                                                      |
|                                                                                                                                                                                                                                                                                                                                    | - Assign DOIs via DataCite for data packages.\                                                                                        |
|                                                                                                                                                                                                                                                                                                                                    | - Embed DOIs in dashboards and metadata.                                                                                              |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| **4. Use Shared Data Models, Ontologies and Metadata to Enable Integration.** Common vocabularies, metadata standards, and ontologies support integration across systems and preserve semantic meaning.                                                                                                                            | \- Adopt ISO 19115, EML, or DataCite metadata standards.\                                                                             |
|                                                                                                                                                                                                                                                                                                                                    | - Re-use terms defined in Salmon Domain Ontology\                                                                                     |
|                                                                                                                                                                                                                                                                                                                                    | - Model datasets using the Darwin Core Data Package Model.                                                                            |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| **5. Store and Analyze Data in Ways That Others Can Easily Access, Use and Trust.** Structured and accessible data formats ease reusability, and support integration with analytical tools and applications while data analyzed or wrangled using programmatic scripts (R, Python etc.) enable reproducibility and increase trust. | \- Provide APIs using FastAPI, Flask, or Django REST.\                                                                                |
|                                                                                                                                                                                                                                                                                                                                    | - Archive in trusted repositories (e.g., GBIF, FRDR, USGS).\                                                                          |
|                                                                                                                                                                                                                                                                                                                                    | - Write scripts in a programming language to wrangle, transform, and analyze data\                                                    |
|                                                                                                                                                                                                                                                                                                                                    | - Use GitHub to host code for collaboration and transparency and the GitHub / Zenodo integration for DOI assignment and preservation. |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| **6. Incentivize and Track Data Sharing and Reuse.** Recognizing data contributors and tracking reuse promotes a culture of sharing and supports professional recognition.                                                                                                                                                         | \- License data with CC-BY 4.0.\                                                                                                      |
|                                                                                                                                                                                                                                                                                                                                    | - Include citation text and visible credit fields.\                                                                                   |
|                                                                                                                                                                                                                                                                                                                                    | - Use COUNTER metrics and DataCite APIs to monitor reuse.\                                                                            |
|                                                                                                                                                                                                                                                                                                                                    | - Encourage dataset citation in references.                                                                                           |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+
| **7. Build Community Through Co-Development and Mutual Benefit.** Engaging users early ensures tools and standards meet real-world needs and enhances long-term stewardship.                                                                                                                                                       | \- Participate in RDA Salmon Interest Group.\                                                                                         |
|                                                                                                                                                                                                                                                                                                                                    | - Facilitate workshops for metadata and vocabulary alignment.\                                                                        |
|                                                                                                                                                                                                                                                                                                                                    | - Support community-engaged research with tangible benefits.                                                                          |
+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------+

: Best practices and practical applications of salmon data stewardship {#tbl-bestpractices}

## Conclusion

Salmon biologists and data stewards across the globe have generated extensive datasets on salmon abundance, environmental conditions, and biological characteristics. When integrated, these data become valuable assets, a fact powerfully demonstrated by studies such as [@peterman2012]. However, as noted by reports to the Cohen Commission [@marmorek2011], these data are often incomplete, inconsistently collected, and fragmented across institutions and jurisdictions. Integrating across such diverse sources can be done, but requires effort that is often not accounted for in smaller-scale studies. This fragmentation is a missed opportunity to deepen our understanding of the drivers of change across salmon life stages and regions, and limits the effectiveness of management decisions, particularly in the face of climate change and biodiversity loss.

But this limitation also reveals an opportunity. By adopting shared best practices in data governance, metadata standardization, persistent identification, infrastructure reuse, and community co-development we can radically improve the transparency, reusability, and interoperability of salmon data. A coordinated, future-oriented data stewardship strategy can transform the role of salmon data in science and management. The case study presented in this paper—drawn from one of the Pacific Region’s most influential salmon survival syntheses [@peterman2012]—illustrates how technical and social data management gaps directly obstructed efforts to answer pressing questions. If some of the best practices we propose had been adopted by the data producers—such as documenting their datasets more thoroughly, storing data in accessible formats, or using persistent identifiers—substantial time and resources could have been saved. The case offers a clear and cautionary tale, as well as a hopeful roadmap.

The emergence of the data stewardship role [@plotkin2014] represents one of the most critical institutional shifts needed to realize this vision. Historically, the work of managing, documenting, and maintaining data has been diffuse and undervalued—often falling to biologists without support, training, or recognition. As the volume and complexity of scientific data grow, so too does the need for clearly defined data stewardship responsibilities embedded within research teams and organizations. Training biologists in the principles and practices of data stewardship—while also supporting dedicated professionals who specialize in this work—is essential to sustaining trustworthy, reusable, and interoperable salmon data systems.

Realizing this vision requires concrete institutional commitments organizations should formally appoint dedicated data stewards with clear roles, responsibilities, and reporting structures. Agencies can adopt centralized metadata repositories and establish compliance metrics to track progress toward FAIR and CARE principles. Key implementation steps include: (1) designating stewardship roles within existing organizational structures, (2) investing in metadata management infrastructure, (3) establishing data governance committees with cross-organization representation, and (4) developing performance indicators that measure data discoverability, interoperability, and reuse. These institutional changes ensure that data stewardship becomes embedded in organizational culture rather than remaining an ad hoc responsibility.

The visionary future state is one where salmon researchers and stewards—across agencies, Indigenous Nations, academic labs, and community groups—can easily access and contribute to well-documented, versioned, and machine-readable datasets. In this future, field biologists, Indigenous guardians, modelers, and policymakers interact with a living knowledge system---one that is flexible, easy to implement, and rooted in principles of FAIRness Indigenous Data Sovereignty. Metadata standards, controlled vocabularies, and shared governance frameworks are not afterthoughts but integral to the culture of data collection and use. Scientists receive credit for publishing high-quality data, and users trust the provenance and structure of the datasets they rely on to make critical management decisions.

Realizing this vision will require investment in both people and systems. Key to this transformation is the emergence of the data steward as a professional role: a hybrid expert who understands operational field biology, information science, governance protocols, and community needs. As highlighted by @roche2020, institutionalizing data stewardship roles ensures long-term capacity for data governance, quality control, and interoperability—functions that are often neglected or left to informal actors. We must not only train new data stewards but also support and upskill biologists to take on stewardship responsibilities in collaborative, interdisciplinary settings. This is essential to address the "technical debt" of unmanaged data and to modernize research practices in line with open science norms. By embedding these practices into the everyday work of data generation, documentation, publication, and reuse, we can move salmon science decisively into the era of data-intensive discovery.

### Competing interests

### Acknowledgements

### References

::: {#refs}
:::

# Appendices

{{< embed appendices/appendix-1-real-world-examples.qmd >}}

{{< embed appendices/appendix-2-training-roadmap.qmd >}}
